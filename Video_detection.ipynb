{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd0e308b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from time import time\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision import datasets, transforms, models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aea3ff70",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict = {1: 'Eurasian_jay',\n",
    " 2: 'great_spotted_woodpecker',\n",
    " 3: 'greenfinch',\n",
    " 4: 'blue_tit',\n",
    " 5: 'Carduelis',\n",
    " 6: 'common_redpoll',\n",
    " 7: 'great_tit',\n",
    " 8: 'bullfinch',\n",
    " 9: 'Eurasian_siskin',\n",
    " 10: 'Eurasian_tree_sparrow',\n",
    " 11: 'hawfinch',\n",
    " 12: 'willow_tit',\n",
    " 13: 'Fieldfare',\n",
    " 14: 'Common chaffinch'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e399ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Russian, English an japanese version of birds Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "834b6b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdsDetection:\n",
    "    \"\"\"\n",
    "    Class implements detection with Faster R-CNN trained for 14 birds species\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, source, out_file = None,\n",
    "                       weights_path = '/home/costia/birds/model_g2400_6_ep.pt',\n",
    "                       conf_lvl = 0.6,labels_dict = labels_dict):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        self.source = source\n",
    "        self.weights_path = weights_path\n",
    "        self.model = self.load_model()\n",
    "        self.out_file = out_file if out_file is not None else source.split('.')[0]+\"_predicted.avi\" \n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.conf_lvl = conf_lvl\n",
    "        self.transform = transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                        ])\n",
    "\n",
    "\n",
    "    def get_video_from_source(self):\n",
    "        \"\"\"\n",
    "        Creates a new video streaming object to extract video frame by frame to make prediction on.\n",
    "        :returns: opencv2 video capture object\n",
    "        \"\"\"\n",
    "        return cv2.VideoCapture(self.source)\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"\n",
    "        load Faster R-CNN without weights pytorch hub and  \n",
    "        load trained weights.\n",
    "               \n",
    "        :returns: Pytorch model with weights.\n",
    "        \"\"\"\n",
    "        #\n",
    "        model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)\n",
    "        in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "        num_classes = 14+1 #(n_classes + background)\n",
    "        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "       #load weights\n",
    "        \n",
    "        model.load_state_dict(torch.load(self.weights_path))\n",
    "        model.eval()\n",
    "        return model\n",
    "\n",
    "    def score_frame(self, frame):\n",
    "        \"\"\"\n",
    "        Takes a single frame as input, and scores the frame .\n",
    "        :param frame: input frame in numpy/list/tuple format.\n",
    "        :returns: Labels and  boxes of objects detected by model in the frame.\n",
    "        \"\"\"\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        inputs = self.transform(frame)\n",
    "        \n",
    "        inputs = (inputs) #for img in frame]\n",
    "        #plt.imshow(inputs)\n",
    "        inputs.unsqueeze_(0)\n",
    "        #print(inputs.shape)\n",
    "        inputs = inputs.to(self.device) #for img in inputs]\n",
    "        with torch.no_grad():\n",
    "            preds = self.model(inputs)\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return preds\n",
    "\n",
    "    def class_to_label(self, label):\n",
    "        \"\"\"\n",
    "        For a given label value, return bird species name.\n",
    "        :param x: numeric label\n",
    "        :return: corresponding bird species\n",
    "        \"\"\"\n",
    "        return labels_dict [label]\n",
    "\n",
    "    def plot_boxes(self, preds, frame):\n",
    "        \"\"\"\n",
    "        Takes a frame and its results as input, and plots the bounding boxes and label on to the frame.\n",
    "        :param results: contains labels and coordinates predicted by model on the given frame.\n",
    "        :param frame: Frame which has been scored.\n",
    "        :returns: Frame with bounding boxes and labels ploted on it.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        mask = preds[0]['scores'] >= self.conf_lvl\n",
    "        print('Founded:',preds[0]['scores'])\n",
    "        confidences = preds[0]['scores'][mask]\n",
    "        boxes = preds[0]['boxes'][mask]\n",
    "        labels = preds[0]['labels'][mask]\n",
    "        \n",
    "        flag = True\n",
    "        \n",
    "        for i, box in enumerate(boxes):\n",
    "            x_min, y_min, x_max, y_max = map(int,box)\n",
    "            bgr = (0, 255, 0)\n",
    "            #print((x_min, y_min), (x_max, y_max))\n",
    "            cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), bgr, 2)\n",
    "            cv2.putText(frame, self.class_to_label(int(labels[i])), (x_min, y_min), cv2.FONT_HERSHEY_SIMPLEX, 0.9, bgr, 2)\n",
    "        return frame\n",
    "      \n",
    "\n",
    "    def __call__(self):\n",
    "        \"\"\"\n",
    "        This function is called when class is executed, it runs the loop to read the video frame by frame,\n",
    "        and write the output into a new file.\n",
    "        :returns: void\n",
    "        \"\"\"\n",
    "        player = self.get_video_from_source()\n",
    "        assert player.isOpened()\n",
    "        x_shape = int(player.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        y_shape = int(player.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        four_cc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "        out = cv2.VideoWriter(self.out_file, four_cc, 30, (x_shape, y_shape))\n",
    "        i = 0\n",
    "        while True:\n",
    "            \n",
    "            start_time = time()\n",
    "            try:\n",
    "                ret, frame = player.read()\n",
    "            except AssertionError:\n",
    "                print(\"Video ended\")\n",
    "                break\n",
    "            assert ret\n",
    "            if i%1==0:\n",
    "                results = self.score_frame(frame)\n",
    "                frame = self.plot_boxes(results, frame)\n",
    "                end_time = time()\n",
    "                fps = 1/np.round(end_time - start_time, 3)\n",
    "                print(f\"Frames Per Second : {fps}\")\n",
    "            \n",
    "            i+=1\n",
    "            out.write(frame)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ff0baf0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Founded: tensor([], device='cuda:0')\n",
      "Frames Per Second : 3.676470588235294\n",
      "Founded: tensor([], device='cuda:0')\n",
      "Frames Per Second : 5.128205128205128\n",
      "Founded: tensor([], device='cuda:0')\n",
      "Frames Per Second : 5.025125628140703\n",
      "Founded: tensor([], device='cuda:0')\n",
      "Frames Per Second : 5.2356020942408374\n",
      "Founded: tensor([], device='cuda:0')\n",
      "Frames Per Second : 4.878048780487805\n",
      "Founded: tensor([], device='cuda:0')\n",
      "Frames Per Second : 5.347593582887701\n",
      "Founded: tensor([], device='cuda:0')\n",
      "Frames Per Second : 4.9504950495049505\n",
      "Founded: tensor([], device='cuda:0')\n",
      "Frames Per Second : 4.784688995215311\n",
      "Founded: tensor([], device='cuda:0')\n",
      "Frames Per Second : 5.208333333333333\n",
      "Founded: tensor([0.0558], device='cuda:0')\n",
      "Frames Per Second : 4.761904761904762\n",
      "Founded: tensor([], device='cuda:0')\n",
      "Frames Per Second : 5.181347150259067\n",
      "Founded: tensor([], device='cuda:0')\n",
      "Frames Per Second : 4.651162790697675\n",
      "Founded: tensor([], device='cuda:0')\n",
      "Frames Per Second : 5.154639175257731\n",
      "Founded: tensor([], device='cuda:0')\n",
      "Frames Per Second : 4.901960784313726\n",
      "Founded: tensor([], device='cuda:0')\n",
      "Frames Per Second : 5.405405405405405\n",
      "Founded: tensor([], device='cuda:0')\n",
      "Frames Per Second : 5.1020408163265305\n",
      "Founded: tensor([], device='cuda:0')\n",
      "Frames Per Second : 5.291005291005291\n",
      "Founded: tensor([], device='cuda:0')\n",
      "Frames Per Second : 5.05050505050505\n",
      "Founded: tensor([], device='cuda:0')\n",
      "Frames Per Second : 5.2356020942408374\n",
      "Founded: tensor([], device='cuda:0')\n",
      "Frames Per Second : 4.926108374384236\n",
      "Founded: tensor([], device='cuda:0')\n",
      "Frames Per Second : 5.347593582887701\n",
      "Founded: tensor([], device='cuda:0')\n",
      "Frames Per Second : 5.181347150259067\n",
      "Founded: tensor([], device='cuda:0')\n",
      "Frames Per Second : 5.291005291005291\n",
      "Founded: tensor([0.1001], device='cuda:0')\n",
      "Frames Per Second : 4.9504950495049505\n",
      "Founded: tensor([0.1059], device='cuda:0')\n",
      "Frames Per Second : 5.2631578947368425\n",
      "Founded: tensor([0.0593, 0.0588], device='cuda:0')\n",
      "Frames Per Second : 4.901960784313726\n",
      "Founded: tensor([], device='cuda:0')\n",
      "Frames Per Second : 5.376344086021505\n",
      "Founded: tensor([], device='cuda:0')\n",
      "Frames Per Second : 4.9504950495049505\n",
      "Founded: tensor([], device='cuda:0')\n",
      "Frames Per Second : 3.2679738562091503\n",
      "Founded: tensor([0.0768], device='cuda:0')\n",
      "Frames Per Second : 4.739336492890995\n",
      "Founded: tensor([0.0615], device='cuda:0')\n",
      "Frames Per Second : 4.566210045662101\n",
      "Founded: tensor([0.1112], device='cuda:0')\n",
      "Frames Per Second : 5.181347150259067\n",
      "Founded: tensor([0.1744, 0.0586], device='cuda:0')\n",
      "Frames Per Second : 4.694835680751174\n",
      "Founded: tensor([0.1230, 0.0664], device='cuda:0')\n",
      "Frames Per Second : 4.524886877828054\n",
      "Founded: tensor([0.1206, 0.0688], device='cuda:0')\n",
      "Frames Per Second : 4.8543689320388355\n",
      "Founded: tensor([0.1858], device='cuda:0')\n",
      "Frames Per Second : 4.62962962962963\n",
      "Founded: tensor([0.2539, 0.1255], device='cuda:0')\n",
      "Frames Per Second : 4.9504950495049505\n",
      "Founded: tensor([0.1295, 0.0732], device='cuda:0')\n",
      "Frames Per Second : 5.2631578947368425\n",
      "Founded: tensor([0.2172, 0.1398, 0.0932], device='cuda:0')\n",
      "Frames Per Second : 5.181347150259067\n",
      "Founded: tensor([0.2786, 0.1605, 0.0567], device='cuda:0')\n",
      "Frames Per Second : 5.025125628140703\n",
      "Founded: tensor([0.2806, 0.2363, 0.1301, 0.0803, 0.0571, 0.0549, 0.0542],\n",
      "       device='cuda:0')\n",
      "Frames Per Second : 4.8076923076923075\n",
      "Founded: tensor([0.2695, 0.1767, 0.1147, 0.0744, 0.0525, 0.0522], device='cuda:0')\n",
      "Frames Per Second : 4.926108374384236\n",
      "Founded: tensor([0.2694, 0.1953, 0.1163, 0.0739, 0.0581, 0.0541, 0.0505],\n",
      "       device='cuda:0')\n",
      "Frames Per Second : 4.9504950495049505\n",
      "Founded: tensor([0.2600, 0.1753, 0.1085, 0.0613], device='cuda:0')\n",
      "Frames Per Second : 4.0\n",
      "Founded: tensor([0.2658, 0.1826, 0.1059, 0.0670], device='cuda:0')\n",
      "Frames Per Second : 4.8543689320388355\n",
      "Founded: tensor([0.2338, 0.2303, 0.1076, 0.0722, 0.0549], device='cuda:0')\n",
      "Frames Per Second : 4.9504950495049505\n",
      "Founded: tensor([0.2533, 0.2106, 0.1002, 0.0720, 0.0532], device='cuda:0')\n",
      "Frames Per Second : 4.878048780487805\n",
      "Founded: tensor([0.4171, 0.1955, 0.1358, 0.0829], device='cuda:0')\n",
      "Frames Per Second : 5.1020408163265305\n",
      "Founded: tensor([0.5621, 0.1376, 0.1075, 0.1000], device='cuda:0')\n",
      "Frames Per Second : 4.672897196261682\n",
      "Founded: tensor([0.6619, 0.1049, 0.0886, 0.0663], device='cuda:0')\n",
      "Frames Per Second : 5.181347150259067\n",
      "Founded: tensor([0.7633, 0.1025, 0.0956, 0.0626], device='cuda:0')\n",
      "Frames Per Second : 5.128205128205128\n",
      "Founded: tensor([0.7395, 0.1055, 0.0817, 0.0532], device='cuda:0')\n",
      "Frames Per Second : 4.901960784313726\n",
      "Founded: tensor([0.6822, 0.1377, 0.0807, 0.0561], device='cuda:0')\n",
      "Frames Per Second : 5.154639175257731\n",
      "Founded: tensor([0.6794, 0.1035, 0.0897, 0.0576], device='cuda:0')\n",
      "Frames Per Second : 5.319148936170213\n",
      "Founded: tensor([0.6827, 0.1361, 0.0838, 0.0554], device='cuda:0')\n",
      "Frames Per Second : 5.319148936170213\n",
      "Founded: tensor([0.5118, 0.1884, 0.0884, 0.0533], device='cuda:0')\n",
      "Frames Per Second : 5.2356020942408374\n",
      "Founded: tensor([0.4028, 0.1320, 0.0600], device='cuda:0')\n",
      "Frames Per Second : 5.319148936170213\n",
      "Founded: tensor([0.3756, 0.1908, 0.1082], device='cuda:0')\n",
      "Frames Per Second : 5.0761421319796955\n",
      "Founded: tensor([0.5069, 0.1741, 0.1073], device='cuda:0')\n",
      "Frames Per Second : 4.716981132075472\n",
      "Founded: tensor([0.5415, 0.1881, 0.0843, 0.0553], device='cuda:0')\n",
      "Frames Per Second : 5.05050505050505\n",
      "Founded: tensor([0.4916, 0.1642, 0.0610], device='cuda:0')\n",
      "Frames Per Second : 5.1020408163265305\n",
      "Founded: tensor([0.6501, 0.1045, 0.0712, 0.0700], device='cuda:0')\n",
      "Frames Per Second : 5.181347150259067\n",
      "Founded: tensor([0.7839, 0.0872], device='cuda:0')\n",
      "Frames Per Second : 5.376344086021505\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_233760/2212478297.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                         \u001b[0mweights_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/costia/birds/best_model_10jan.pt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                         )\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_233760/3348066058.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                 \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_boxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_233760/3348066058.py\u001b[0m in \u001b[0;36mscore_frame\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#for img in inputs]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DL/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DL/lib/python3.8/site-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroi_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_image_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DL/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DL/lib/python3.8/site-packages/torchvision/models/detection/rpn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, features, targets)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0mobjectness\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_bbox_deltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m         \u001b[0manchors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manchor_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0mnum_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DL/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DL/lib/python3.8/site-packages/torchvision/models/detection/anchor_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image_list, feature_maps)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mimage_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         strides = [[torch.tensor(image_size[0] // g[0], dtype=torch.int64, device=device),\n\u001b[0m\u001b[1;32m    123\u001b[0m                     torch.tensor(image_size[1] // g[1], dtype=torch.int64, device=device)] for g in grid_sizes]\n\u001b[1;32m    124\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_cell_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DL/lib/python3.8/site-packages/torchvision/models/detection/anchor_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mimage_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         strides = [[torch.tensor(image_size[0] // g[0], dtype=torch.int64, device=device),\n\u001b[0m\u001b[1;32m    123\u001b[0m                     torch.tensor(image_size[1] // g[1], dtype=torch.int64, device=device)] for g in grid_sizes]\n\u001b[1;32m    124\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_cell_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create a new birds detection object and execute.\n",
    "detect = BirdsDetection(source = \"/home/costia/videoplayback_360.mp4\",conf_lvl=0.5,\n",
    "                        weights_path = '/home/costia/birds/best_model_10jan.pt',\n",
    "                        )\n",
    "detect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
